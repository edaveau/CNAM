{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part below allows to retrieve the necessary packages in order for the script to run correctly. We also included the APK which allows us to connect to the Open Data SNCF API. You may remove the # at the beginning of the script should you need to install packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install requests;\n",
    "#!{sys.executable} -m pip install json;\n",
    "#!{sys.executable} -m pip install pandas;\n",
    "#!{sys.executable} -m pip install datetime;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "import json as j\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pandas.io.json import json_normalize\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "user = \"PASTE YOUR API KEY HERE\"\n",
    "coverage = \"fr-idf\"\n",
    "url_stations = \"https://api.sncf.com/v1/coverage/sncf/physical_modes/physical_mode%3ALongDistanceTrain/stop_points//?count=1000&\"\n",
    "url_lines = \"https://api.sncf.com/v1/coverage/sncf/physical_modes/physical_mode%3ALongDistanceTrain/lines//?count=1000&\"\n",
    "url_theoTimes = \"https://api.sncf.com/v1/coverage/sncf/physical_modes/physical_mode%3ALongDistanceTrain/vehicle_journeys//?count=1000&since=20190320T205354&\"\n",
    "url_realTimes = \"https://api.sncf.com/v1/coverage/sncf/physical_modes/physical_mode%3ALongDistanceTrain/disruptions//?count=1000&since=20190320T205354&\"\n",
    "headers={'Authorization': 'TOK:apk'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we request the data from the SNCF API. The response codes are shown below to ensure we correctly retrieved each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stations = r.get(url_stations, \n",
    "                 auth=HTTPBasicAuth(user, \"\"))\n",
    "response_lines = r.get(url_lines, \n",
    "                 auth=HTTPBasicAuth(user, \"\"))\n",
    "response_theoTimes = r.get(url_theoTimes, \n",
    "                 auth=HTTPBasicAuth(user, \"\"))\n",
    "response_realTimes = r.get(url_realTimes, \n",
    "                 auth=HTTPBasicAuth(user, \"\"))\n",
    "print(\" Response code for the stations request: \", response_stations.status_code, \"\\n\",\n",
    "    \"Response code for the lines request: \", response_lines.status_code, \"\\n\",\n",
    "    \"Response code for the theoretical times request: \", response_theoTimes.status_code, \"\\n\",\n",
    "    \"Response code for the real times request: \", response_realTimes.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can load the data in a dictionary on Python. We also print the keys in order to have a view on the relevance of the data we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_stations = j.loads(response_stations.text)\n",
    "json_lines = j.loads(response_lines.text)\n",
    "json_theoTimes = j.loads(response_theoTimes.text)\n",
    "json_realTimes = j.loads(response_realTimes.text)\n",
    "print(\"Class and main keys from the stations data : \", type(json_stations))\n",
    "for key, value in json_stations.items() :\n",
    "    print (key)\n",
    "\n",
    "print(\"\\nClass and main keys from the lines data : \", type(json_lines))\n",
    "for key, value in json_lines.items() :\n",
    "    print (key)\n",
    "    \n",
    "print(\"\\nClass and main keys from the theoretical times data : \", type(json_theoTimes))\n",
    "for key, value in json_theoTimes.items() :\n",
    "    print (key)\n",
    "    \n",
    "print(\"\\nClass and main keys from the real times data : \", type(json_realTimes))\n",
    "for key, value in json_realTimes.items() :\n",
    "    print (key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, data were correctly retrieved in JSON format, which we then converted to a dictionary format. The main issue is that all the records of interest are respectively contained within the \"stop_points\", \"lines\", \"vehicle_journeys\" and \"disruptions\" sub-dictionaries. Therefore, we first need to extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove_stations = [\"pagination\", \"links\", \"disruptions\", \"feed_publishers\", \"context\"]\n",
    "keys_to_remove_lines = [\"pagination\", \"links\", \"disruptions\", \"feed_publishers\", \"context\"]\n",
    "keys_to_remove_theoTimes = [\"pagination\", \"links\", \"disruptions\", \"feed_publishers\", \"context\"]\n",
    "keys_to_remove_realTimes = [\"pagination\", \"links\", \"feed_publishers\", \"context\"]\n",
    "for k in keys_to_remove_stations :\n",
    "    json_stations.pop(k, None)\n",
    "for k in keys_to_remove_lines :\n",
    "    json_stations.pop(k, None)\n",
    "for k in keys_to_remove_theoTimes :\n",
    "    json_stations.pop(k, None)\n",
    "for k in keys_to_remove_realTimes :\n",
    "    json_stations.pop(k, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have correctly removed irrelevant data from the main dictionary. However, the subdictionaries, which are the data of interest are contained within a list. Therefore, we first need to extract the list in order to manipulate more easily each nested dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = []\n",
    "lines = []\n",
    "theoTimes = []\n",
    "realTimes = []\n",
    "for station in json_stations[\"stop_points\"]:\n",
    "    stations.append(station)\n",
    "for line in json_lines[\"lines\"]:\n",
    "    lines.append(line)\n",
    "for theoTime in json_theoTimes[\"vehicle_journeys\"]:\n",
    "    theoTimes.append(theoTime)\n",
    "for realTime in json_realTimes[\"disruptions\"]:\n",
    "    realTimes.append(realTime)\n",
    "data_sample2 = [stations[0], lines[0], theoTimes[0], realTimes[0]]\n",
    "data_sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in realTimes[0].items() :\n",
    "    print (key)\n",
    "realTimes[6][\"impacted_objects\"][0][\"impacted_stops\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect ! We managed to print relevant data ! We just need to append each element to a list, and the job will be almost done !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stations = []\n",
    "list_of_lines = []\n",
    "list_of_routes = []\n",
    "list_of_vehicle_patterns = []\n",
    "list_of_delayed_times = []\n",
    "\n",
    "for i in stations:\n",
    "    timestamp = str(datetime.datetime.now()).split('.')[0]\n",
    "    a = [i[\"id\"], i[\"label\"], i[\"coord\"][\"lon\"], i[\"coord\"][\"lat\"], timestamp]\n",
    "    list_of_stations.append(a)\n",
    "\n",
    "for i in lines:\n",
    "    timestamp = str(datetime.datetime.now()).split('.')[0]\n",
    "    a = [i[\"id\"], i[\"name\"], i[\"commercial_mode\"][\"name\"], timestamp]\n",
    "    list_of_lines.append(a)\n",
    "\n",
    "for i in lines:\n",
    "    timestamp = str(datetime.datetime.now()).split('.')[0]\n",
    "    for path in i[\"routes\"] :\n",
    "        a = [path[\"id\"], path[\"name\"], i[\"id\"], i[\"routes\"][0][\"direction\"][\"stop_area\"][\"codes\"][-2][\"value\"], \n",
    "        i[\"routes\"][-1][\"direction\"][\"stop_area\"][\"codes\"][-2][\"value\"], timestamp]\n",
    "        list_of_routes.append(a)    \n",
    "\n",
    "for i in theoTimes:\n",
    "    timestamp = str(datetime.datetime.now()).split('.')[0]\n",
    "    a = [i[\"trip\"][\"id\"], i[\"id\"], i[\"calendars\"][0][\"week_pattern\"][\"monday\"], i[\"calendars\"][0][\"week_pattern\"][\"tuesday\"], \n",
    "         i[\"calendars\"][0][\"week_pattern\"][\"wednesday\"], i[\"calendars\"][0][\"week_pattern\"][\"thursday\"], \n",
    "         i[\"calendars\"][0][\"week_pattern\"][\"friday\"], i[\"calendars\"][0][\"week_pattern\"][\"saturday\"], \n",
    "         i[\"calendars\"][0][\"week_pattern\"][\"sunday\"]]\n",
    "    for time in i[\"stop_times\"] :\n",
    "        b = [time[\"stop_point\"][\"id\"], time[\"arrival_time\"], time[\"departure_time\"]]\n",
    "        c = [i[\"stop_times\"][0][\"stop_point\"][\"id\"], i[\"stop_times\"][-1][\"stop_point\"][\"id\"], timestamp]\n",
    "        b = b+c\n",
    "    a = a+b\n",
    "    list_of_vehicle_patterns.append(a)\n",
    "            \n",
    "for i in realTimes :\n",
    "    timestamp = str(datetime.datetime.now()).split('.')[0]\n",
    "    for impact in i[\"impacted_objects\"] :\n",
    "        a = [i[\"updated_at\"], i[\"impact_id\"], impact[\"pt_object\"][\"id\"]]\n",
    "        for stop in impact[\"impacted_stops\"] :\n",
    "            dep_time = stop.get(\"amended_departure_time\", None)\n",
    "            arr_time = stop.get(\"amended_arrival_time\", None)\n",
    "            base_dep_time = stop.get(\"base_departure_time\", None)\n",
    "            base_arr_time = stop.get(\"base_arrival_time\", None)\n",
    "            b = [stop[\"stop_point\"][\"id\"], stop[\"departure_status\"], stop[\"arrival_status\"], \n",
    "                  stop[\"cause\"], dep_time, arr_time, base_dep_time, base_arr_time, timestamp]\n",
    "        a = a+b\n",
    "        list_of_delayed_times.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_stations = [\"station_ref\", \"station_name\", \"station_longitude\", \"station_latitude\", \"last_update\"]\n",
    "labels_lines = [\"line_ref\", \"line_name\", \"line_commercial_mode\", \"last_update\"]\n",
    "labels_routes = [\"route_ref\", \"route_name\", \"line_ref\", \"start_station_ref\", \"destination_station_ref\", \"last_update\"]\n",
    "labels_vehicle_pattern = [\"vehicle_journeys_ref\", \"vehicle_journeys_section_ref\", \"week_pattern_monday\", \"week_pattern_tuesday\", \"week_pattern_wednesday\",\n",
    "                          \"week_pattern_thursday\", \"week_pattern_friday\", \"week_pattern_saturday\", \"week_pattern_sunday\", \n",
    "                          \"station_ref\", \"station_arrival_time\", \"station_departure_time\", \"station_name_dep\", \n",
    "                          \"station_name_arr\", \"last_update\"]\n",
    "labels_delayed_times = [\"disruption_date\", \"disruption_ref\", \"impacted_vehicle_journeys\", \"impacted_arrival_station\", \n",
    "                        \"departure_status\", \"arrival_status\", \"delay_cause\", \"amended_departure_time\",\n",
    "                        \"amended_arrival_time\", \"base_departure_time\", \"base_arrival_time\", \"last_update\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data are almost ready, we will convert the lists in dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = pd.DataFrame.from_records(list_of_stations, columns = labels_stations)\n",
    "lines = pd.DataFrame.from_records(list_of_lines, columns = labels_lines)\n",
    "routes = pd.DataFrame.from_records(list_of_routes, columns = labels_routes)\n",
    "vehicle_pattern = pd.DataFrame.from_records(list_of_vehicle_patterns, columns = labels_vehicle_pattern)\n",
    "disruptions = pd.DataFrame.from_records(list_of_delayed_times, columns = labels_delayed_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station.station_ref = station.station_ref.replace({r'stop_point\\:OCE\\:SP\\:':''}, regex = True)\n",
    "\n",
    "lines.line_ref = lines.line_ref.replace({r'line\\:OCE\\:' : ''}, regex = True)\n",
    "\n",
    "routes.line_ref = routes.line_ref.replace({r'line\\:OCE\\:' : ''}, regex = True)\n",
    "routes.route_ref = routes.route_ref.replace({r'route\\:OCE\\:' : ''}, regex = True)\n",
    "\n",
    "vehicle_pattern.vehicle_journeys_ref = vehicle_pattern.vehicle_journeys_ref.replace({\n",
    "    r'OCE\\:':''}, regex = True)\n",
    "vehicle_pattern.vehicle_journeys_section_ref = vehicle_pattern.vehicle_journeys_section_ref.replace({\n",
    "    r'vehicle_journey\\:OCE\\:':''}, regex = True)\n",
    "vehicle_pattern.station_ref = vehicle_pattern.station_ref.replace({r'stop_point\\:OCE\\:SP\\:':''}, regex = True)\n",
    "vehicle_pattern.station_name_dep = vehicle_pattern.station_name_dep.replace({r'stop_point\\:OCE\\:SP\\:':''}, regex = True)\n",
    "vehicle_pattern.station_name_arr = vehicle_pattern.station_name_arr.replace({r'stop_point\\:OCE\\:SP\\:':''}, regex = True)\n",
    "\n",
    "disruptions.impacted_vehicle_journeys = disruptions.impacted_vehicle_journeys.replace({r'OCE\\:':''}, regex = True)\n",
    "disruptions.impacted_arrival_station =disruptions .impacted_arrival_station.replace({r'stop_point\\:OCE\\:SP\\:':''}, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coltypes = print(station.dtypes, \"\\n\", \"\\n\",lines.dtypes, \"\\n\", \"\\n\", routes.dtypes, \"\\n\", \"\\n\",\n",
    "                 vehicle_pattern.dtypes, \"\\n\", \"\\n\", disruptions.dtypes)\n",
    "coltypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station[[\"station_latitude\",\n",
    "         \"station_longitude\"]] = station[[\"station_latitude\", \"station_longitude\"]].apply(pd.to_numeric)\n",
    "\n",
    "routes[[\"start_station_ref\", \"destination_station_ref\"]] = routes[[\"start_station_ref\", \"destination_station_ref\"]].apply(pd.to_numeric)\n",
    "\n",
    "vehicle_pattern['station_arrival_time'] = pd.to_datetime(vehicle_pattern['station_arrival_time'],format= '%H%M%S').dt.time\n",
    "vehicle_pattern['station_departure_time'] = pd.to_datetime(vehicle_pattern['station_departure_time'],format= '%H%M%S').dt.time\n",
    "\n",
    "disruptions['disruption_date'] = pd.to_datetime(disruptions['disruption_date'],format= '%Y%m%dT%H%M%S' )\n",
    "disruptions['amended_departure_time'] = pd.to_datetime(disruptions['amended_departure_time'],format= '%H%M%S' ).dt.time\n",
    "disruptions['amended_arrival_time'] = pd.to_datetime(disruptions['amended_arrival_time'],format= '%H%M%S' ).dt.time\n",
    "disruptions['base_departure_time'] = pd.to_datetime(disruptions['base_departure_time'],format= '%H%M%S' ).dt.time\n",
    "disruptions['base_arrival_time'] = pd.to_datetime(disruptions['base_arrival_time'],format= '%H%M%S' ).dt.time\n",
    "\n",
    "station[\"last_update\"] = pd.to_datetime(station['last_update'],format= '%Y-%m-%d %H:%M:%S')\n",
    "lines[\"last_update\"] = pd.to_datetime(station['last_update'],format= '%Y-%m-%d %H:%M:%S')\n",
    "routes[\"last_update\"] = pd.to_datetime(station['last_update'],format= '%Y-%m-%d %H:%M:%S')\n",
    "vehicle_pattern[\"last_update\"] = pd.to_datetime(station['last_update'],format= '%Y-%m-%d %H:%M:%S')\n",
    "disruptions[\"last_update\"] = pd.to_datetime(station['last_update'],format= '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coltypes = print(station.dtypes, \"\\n\", \"\\n\",lines.dtypes, \"\\n\", \"\\n\", routes.dtypes, \"\\n\", \"\\n\",\n",
    "                 vehicle_pattern.dtypes, \"\\n\", \"\\n\", disruptions.dtypes)\n",
    "coltypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_pattern.station_name_dep = vehicle_pattern.station_name_dep.replace({r'stop_point\\:OCE\\:SP\\:':''}, regex = True)\n",
    "vehicle_pattern.station_name_arr = vehicle_pattern.station_name_arr.replace({r'[^0-9.]':''}, regex = True)\n",
    "\n",
    "vehicle_pattern[\"route_ref\"] = vehicle_pattern.apply(lambda row: row.station_name_dep + \"-\" + row.station_name_arr, axis=1)\n",
    "vehicle_pattern = vehicle_pattern.drop('station_name_dep', 1)\n",
    "vehicle_pattern = vehicle_pattern.drop('station_name_arr', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_pattern = vehicle_pattern[[c for c in vehicle_pattern if c not in ['last_update']] \n",
    "       + ['last_update']]\n",
    "vehicle_pattern.station_ref = vehicle_pattern.station_ref.replace({r'[^0-9.]':''}, regex = True)\n",
    "station.station_ref = station.station_ref.replace({r'[^0-9.]':''}, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = station.drop_duplicates([\"station_ref\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose to display any table to have a view of the data. Please type any name of dataframe (station, lines, routes, vehicle_pattern, disruptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = routes.replace({\"last_update\":{None : timestamp}}).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install sqlalchemy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://postgres:admin@localhost:5432/SNCF')\n",
    "lines.to_sql('lines', engine, if_exists = \"append\", index = False, schema = \"rail_network\")\n",
    "station.to_sql('stations', engine, schema = \"rail_network\", if_exists = \"append\", index = False)\n",
    "routes.to_sql('routes', engine, if_exists = \"append\", index = False, schema = \"rail_network\")\n",
    "vehicle_pattern.to_sql('vehicle_pattern', engine, if_exists = \"append\", index = False)\n",
    "disruptions.to_sql('disruptions', engine, if_exists = \"append\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
